# Wide & Deep 모델 설명 가이드

MF와 NCF가 "취향의 패턴"을 찾는 데 집중했다면, Wide & Deep은 **"패턴(Deep)"과 "암기(Wide)"를 동시에** 하려는 욕심쟁이 모델입니다.
구글(Google)이 Play Store 추천을 위해 만든 아주 유명한 모델입니다.

## 1. 핵심 컨셉: "암기(Memorization) vs 일반화(Generalization)"

이 모델은 두 명의 전문가가 협업하는 구조입니다.

### (1) Wide 모델 (암기왕 - 꼰대 사서)
-   **특징**: 있는 그대로 외웁니다. 융통성이 없습니다.
-   **잘하는 것**: "치킨을 산 사람은 맥주를 산다" (공식처럼 외움)
-   **단점**: "치킨" 대신 "닭강정"을 사면? "어? 이건 내가 아는 공식에 없는데?" 하고 모릅니다.

### (2) Deep 모델 (응용왕 - 신입 AI)
-   **특징**: 비슷하면 묶어서 생각합니다. (우리가 배운 NCF/MLP)
-   **잘하는 것**: "치킨이나 닭강정이나 튀긴 닭이니까, 맥주랑 어울리겠네." (추론)
-   **단점**: 가끔 너무 맘대로 생각해서 엉뚱한 추천을 할 때가 있습니다. (예: "튀긴 건 다 맥주랑 어울려!" -> "감자튀김" (O), "돈까스" (O), "튀김소보로" (?) )

## 2. 왜 합쳤을까? (The Power of Hybrid)

> **"뻔한 건 확실하게(Wide), 새로운 건 똑똑하게(Deep) 추천하자!"**

-   **Wide**: "철수는 `아이언맨 1`을 봤으니 `아이언맨 2`를 무조건 보여줘!" (확실한 규칙)
-   **Deep**: "철수는 히어로물을 좋아하니까, 이번에 새로 나온 `가디언즈 오브 갤럭시`도 좋아할 거야." (취향 확장)

이 둘을 합치면(Wide & Deep), **"실수하지 않으면서도(Wide), 센스 있는(Deep)"** 추천이 가능해집니다.

## 3. 구조 (Architecture)

```python
# 1. Wide Part (Linear)
# 입력: "장르=액션" AND "배우=톰크루즈" (Cross Product)
wide_output = Linear(features)

# 2. Deep Part (MLP)
# 입력: 모든 정보 (임베딩) -> Layer 1 -> Layer 2
deep_output = MLP(embeddings)

# 3. 결합 (Combine)
final_score = Sigmoid(wide_output + deep_output)
```

## 5. Q&A: "이 정도면 SQL로도 만들 수 있지 않을까?"

**반은 맞고 반은 틀립니다!** 아주 날카로운 지적입니다.

### 1. Wide 부분은 SQL로 됩니다! (Memorization)
-   **Wide 모델의 본질**: "A를 산 사람이 B도 샀다" (통계)
-   **SQL**: `SELECT ... GROUP BY ... COUNT` 하면 끝입니다.
-   **한계**: "치킨"을 산 사람에게 "맥주"는 추천할 수 있지만, **"닭강정"을 산 사람에게는 아무것도 추천 못 합니다.** (DB에 "닭강정-맥주" 기록이 없으면 끝이니까요.)

### 2. Deep 부분은 SQL로 안 됩니다! (Generalization)
-   **Deep 모델의 본질**: "치킨이랑 닭강정은 비슷하네?" (임베딩/추론)
-   **Model**: 벡터 공간에서 거리를 계산해서 **기록에 없는 조합**도 찾아냅니다.
-   **능력**: "닭강정-맥주" 기록이 0건이어도, "치킨-맥주" 관계를 응용해서 추천해줍니다.

### 3. 병렬성(Parallelism)은 덤입니다
-   질문하신 대로 GPU를 써서 **병렬 연산**이 빠른 것도 장점입니다.
-   하지만 더 큰 장점은 **"없는 것을 유추해내는 능력(Generalization)"**입니다. SQL은 "검색"이고, 모델은 "예측"입니다.

## 6. Q&A: "결국에는 벡터에서 제일 가까운 점수를 주는 거 아니야?"

**MF는 맞지만, Deep 모델은 아닙니다!** 이 차이가 핵심입니다.

### 1. MF = 거리 재기 (Distance)
-   **방식**: "철수랑 영희랑 가깝네? 그럼 영희가 본 거 추천!"
-   **핵심**: **유사도(Similarity)**
-   **비유**: 지도에서 가까운 식당 찾기.

### 2. Deep Model = 궁합 보기 (Complex Function)
-   **방식**: "철수랑 영희는 가깝지만(성격 비슷), 둘 다 리더십이 강해서 같은 팀 하면 싸울 거야." (비선형적 관계)
-   **핵심**: **적합도(Compatibility)**
-   **비유**: 소개팅 주선자가 성격, 취미, 혈액형 다 따져보고 "이 둘은 천생연분!"이라고 판단하는 것.

### 결론
단순히 **"가까운 것"**만 찾는 게 아니라, **"복잡한 조건(Logic)"**을 통과한 최적의 아이템을 찾는 것입니다. 그래서 딥러닝이 더 강력한 것입니다.

## 7. Q&A: "그렇다면 복잡한 조건들도 SQL로도 충분히 꺼내올 수 있는 거 아니야?"

**이론상으로는 가능하지만, 현실적으로는 불가능합니다.** (Human Resource의 한계)

### 1. SQL (수동 운전)
-   복잡한 조건을 **사람이 일일이 다 짜야 합니다**.
-   `WHERE age > 20 AND genre = 'Action' AND (view_time > 30 OR click_count > 5) ...`
-   **문제점**:
    1.  조건이 100개, 1000개가 되면 사람이 짤 수 없습니다.
    2.  **"우리가 모르는 규칙"**은 짤 수 없습니다. (예: "비 오는 날에는 공포 영화를 30대 남성이 좋아한다"는 걸 사람이 어떻게 알까요?)

### 2. Deep Model (자율 주행)
-   모델이 데이터를 보고 **규칙을 스스로 찾아냅니다**.
-   **능력**: "데이터를 보니까, 비 오는 날 30대 남성이 공포 영화를 많이 보네? 내가 이 규칙을 추가해야지." (자동 학습)
-   **결론**: SQL로 하려면 **수천 명의 개발자**가 매일 밤새서 `IF-ELSE` 문을 짜야 하는 일을, 모델은 혼자서 해냅니다.

## 9. Q&A: "CrossEntropyLoss랑 MSELoss는 뭐가 다른 거야?"

**"푸는 문제의 종류"**가 다릅니다.

### 1. MSELoss (Mean Squared Error)
-   **용도**: **회귀 (Regression)** - "얼마나?"를 맞출 때
-   **상황**: Wide & Deep 모델 (평점 예측)
-   **비유**: **"양궁 과녁 맞추기"**
    -   정답(4.5점)에서 내 화살(3.0점)이 **얼마나 멀리 벗어났는가(거리)**를 잽니다.
    -   멀리 벗어날수록 제곱으로 더 많이 혼납니다.

### 2. CrossEntropyLoss
-   **용도**: **분류 (Classification)** - "누구?"를 맞출 때
-   **상황**: SASRec 모델 (다음 영화 맞추기)
-   **비유**: **"객관식 시험 찍기"**
    -   정답(3번)을 찍을 **확률**을 얼마나 높게 불렀는가를 봅니다.
    -   거리가 아니라, **"정답에 대한 확신"**을 채점합니다.

### 요약
-   **별점(1~5점)을 맞추고 싶다?** -> `MSELoss` (숫자 차이를 줄여라!)
-   **다음 영화(10,000개 중 하나)를 맞추고 싶다?** -> `CrossEntropyLoss` (정답 확률을 높여라!)

## 10. 면접용 요약 멘트

> "Wide & Deep 모델은 **'암기(Memorization)'**에 강한 선형 모델(Wide)과 **'일반화(Generalization)'**에 강한 신경망 모델(Deep)을 결합한 하이브리드 모델입니다.
> Wide 부분은 '치킨-맥주' 같은 빈번한 공기(Co-occurrence) 패턴을 학습하고, Deep 부분은 임베딩을 통해 보지 못한 조합까지 추론합니다.
> 이를 통해 추천의 **다양성(Serendipity)**과 **정확성(Accuracy)**을 동시에 잡았습니다."

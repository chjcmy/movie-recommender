# NCF (Neural Collaborative Filtering) 설명 가이드

MF(Matrix Factorization)가 "직선"이라면, NCF는 "곡선"입니다.
더 복잡하고 미묘한 취향을 잡아내기 위해 딥러닝(Neural Network)을 도입한 모델입니다.

## 1. MF의 한계: "선형적(Linear)이다"
MF는 두 벡터를 곱하기만(Dot Product) 합니다. 이는 수학적으로 **선형 결합**이라고 합니다.
-   **문제점**: 사람의 취향은 단순하지 않습니다.
    -   *"액션을 좋아하면 로맨스를 싫어한다"* (MF가 잘함)
    -   *"액션을 좋아하는데, 로맨스가 섞이면 싫어하지만, 주연 배우가 톰 크루즈면 로맨스가 있어도 본다"* (MF가 힘들어함)

## 2. NCF의 해결책: "비선형성(Non-linearity) 추가"
NCF는 **다층 퍼셉트론(MLP)**이라는 신경망을 추가해서 이 문제를 해결합니다.

### 구조 (Architecture)
우리 코드(`algorithms.py`)에 구현된 NCF는 정확히는 **NeuMF (Neural Matrix Factorization)** 구조입니다. 두 가지 길을 합쳤기 때문입니다.

1.  **왼쪽 길 (GMF)**: 기존의 MF처럼 곱하기를 합니다. (전통적인 강점 유지)
2.  **오른쪽 길 (MLP)**: 벡터를 합친 뒤(Concat), 여러 층의 레이어를 통과시킵니다.
    -   `Layer 1` -> `Layer 2` -> `Layer 3` ...
    -   이 과정을 거치면서 모델은 **"복잡한 패턴"**을 학습합니다. (예: XOR 문제 같은 비선형 관계)

## 3. 비유: "요리사 vs AI 셰프"

-   **MF (요리사)**: 정해진 레시피대로 재료를 섞습니다. (소금 * 0.5 + 설탕 * 0.2)
    -   단순하고 빠르지만, 창의적인 맛을 내긴 어렵습니다.
-   **NCF (AI 셰프)**: 재료를 섞은 뒤, 끓이고, 볶고, 숙성시킵니다. (Layer 통과)
    -   재료들 간의 미묘한 화학 반응(상호작용)까지 찾아내어 훨씬 깊은 맛을 냅니다.

## 4. Q&A: "MLP가 정확히는 뭐지?"

**MLP (Multi-Layer Perceptron)**는 우리말로 **"다층 퍼셉트론"**이라고 하며, 가장 기본적인 형태의 인공신경망입니다.
쉽게 말해 **"여러 단계의 결재 시스템(위원회)"**라고 생각하시면 됩니다.

### 작동 원리 (Committee Analogy)
사용자 정보와 영화 정보가 들어오면, 여러 층의 위원회를 거치며 심사를 받습니다.

1.  **입력층 (Input)**: 철수(액션팬)와 어벤져스(액션영화) 데이터가 들어옵니다.
2.  **은닉층 1 (Hidden Layer 1 - 실무진)**:
    -   "장르가 맞나?" (O)
    -   "배우가 유명한가?" (O)
    -   "철수가 어제 본 영화랑 비슷한가?" (O)
    -   이런 수백 개의 질문을 던지고 점수를 매깁니다.
3.  **은닉층 2 (Hidden Layer 2 - 부장급)**:
    -   실무진의 보고서를 바탕으로 더 복잡한 판단을 합니다.
    -   "장르는 맞는데, 철수가 요즘 너무 액션만 봐서 질리지 않았을까?" (비선형적 사고)
4.  **출력층 (Output - 사장님)**:
    -   최종 결재 도장을 찍습니다. **"추천 점수: 4.8점!"**

### 왜 쓰는가?
MF처럼 단순히 곱하기만 하면 "액션 많이 봐서 질린 상태" 같은 미묘한 상황을 캐치하기 어렵습니다.
MLP는 층을 거치면서 이런 **"복잡하고 꼬여있는 조건"**들을 학습할 수 있습니다.

## 5. Q&A: "NCF를 사용하려면 MF보다 더 많은 정보가 필요하겠구나?"

**입력 정보(종류)는 같지만, 데이터 양(개수)은 더 많이 필요합니다.**

### 1. 입력 정보는 동일합니다 (Features)
-   MF와 마찬가지로 NCF도 **User ID**와 **Item ID**만 있으면 돌아갑니다.
-   추가적인 정보(나이, 장르 등)가 필수는 아닙니다.

### 2. 하지만 "공부할 문제집"은 더 많이 필요합니다 (Data Volume)
-   **MF (단순한 학생)**: 공식이 간단해서 문제집을 조금만 풀어도 금방 배웁니다.
-   **NCF (똑똑하지만 의심 많은 학생)**:
    -   뇌 구조(MLP)가 복잡해서, 아주 미묘한 패턴까지 찾으려고 합니다.
    -   데이터가 적으면 "이건 우연인가? 진짜인가?" 헷갈려하다가 엉뚱한 것을 외워버릴 수 있습니다 (**과적합, Overfitting**).
    -   그래서 **더 많은 평점 데이터**가 있어야 제 성능을 발휘합니다.

## 6. Q&A: "그러면 데이터가 얼마나 있어야 MF/NCF를 추천하나?"

정확한 "숫자"는 없지만, 현업에서의 일반적인 기준(Rule of Thumb)은 다음과 같습니다.

### 1. MF를 추천하는 경우 (Small & Sparse)
-   **데이터 규모**: 평점 10만 개 미만 ~ 100만 개 정도
-   **상황**: 스타트업 초기, 유저 수가 적을 때
-   **이유**:
    -   데이터가 적을 때 딥러닝(NCF)을 쓰면 **과적합(Overfitting)**되기 쉽습니다.
    -   MF는 구조가 단순해서 적은 데이터로도 안정적인 성능을 냅니다.
    -   학습과 서빙 속도가 훨씬 빠릅니다.

### 2. NCF를 추천하는 경우 (Large & Dense)
-   **데이터 규모**: 평점 100만 개 이상 (수천만 개~)
-   **상황**: 넷플릭스, 유튜브, 쿠팡 같은 대규모 서비스
-   **이유**:
    -   데이터가 충분히 많아서 복잡한 패턴을 학습할 수 있습니다.
    -   MF가 놓치는 미묘한 비선형적 관계까지 잡아내어 정확도를 1~2%라도 더 올릴 수 있습니다. (대기업에선 이 1%가 엄청난 매출 차이입니다.)

### 결론
> "일단 **MF로 시작**해서 기준점(Baseline)을 잡고, 데이터가 쌓이면 **NCF로 고도화**하는 것이 정석입니다."

## 7. Q&A: "코드의 수학식은 어떻게 되나?"

질문하신 코드는 **"한 층(Layer)의 연산"**을 정의하고 있습니다. 이를 수식으로 풀면 다음과 같습니다.

```python
mlp_modules.append(nn.Linear(input_size, output_size))
mlp_modules.append(nn.ReLU())
mlp_modules.append(nn.Dropout(0.2))
```

### 수학적 표현
입력 벡터를 $x$, 출력 벡터를 $y$라고 할 때:

$$
y = \text{Dropout}(\text{ReLU}(Wx + b))
$$

### 단계별 상세 설명

1.  **`nn.Linear(input, output)`**: 선형 변환 (Linear Transformation)
    -   입력 $x$에 가중치 행렬 $W$를 곱하고 편향 $b$를 더합니다.
    -   $$z = Wx + b$$
    -   *의미: 입력 데이터의 특징을 섞고 변형하여 새로운 특징을 만들어냅니다.*

2.  **`nn.ReLU()`**: 활성화 함수 (Activation Function)
    -   0보다 작은 값은 0으로 만들고, 0보다 큰 값은 그대로 둡니다.
    -   $$a = \max(0, z)$$
    -   *의미: 선형적인 계산에 "비선형성(꺾임)"을 주어 복잡한 패턴을 이해하게 합니다.*

3.  **`nn.Dropout(0.2)`**: 드롭아웃 (Regularization)
    -   확률 $p=0.2$로 뉴런을 랜덤하게 꺼버립니다(0으로 만듦).
    -   $$y = r \odot a \quad (r \sim \text{Bernoulli}(1-p))$$
    -   *의미: 특정 뉴런에 너무 의존하지 않게 하여, 과적합(Overfitting)을 막고 시험에 강한 모델을 만듭니다.*

## 8. Q&A: "그럼 ReLU는 의도적으로 꺾임을 주기 위해서 쓰는 거구나?"

**정확합니다!** 그 "꺾임"이 없으면 딥러닝은 아무리 층을 깊게 쌓아도 바보가 됩니다.

### 왜 "꺾임(비선형성)"이 필요한가?
세상에는 **직선 하나로 해결할 수 없는 문제**가 너무 많기 때문입니다.

#### 1. 직선의 한계 (Linear)
-   종이에 빨간 점과 파란 점이 섞여 있다고 상상해보세요.
-   만약 점들이 복잡하게 섞여 있다면(예: 태극 문양), **직선 하나(Linear)**를 아무리 잘 그어도 완벽하게 나눌 수 없습니다.
-   `Linear` + `Linear` + `Linear` = 결국 그냥 **기울기가 다른 직선 하나**일 뿐입니다.

#### 2. 꺾임의 마법 (Non-linear / ReLU)
-   ReLU를 쓴다는 건, 종이를 **"접는 것"**과 같습니다.
-   종이를 접고, 구기고, 비틀면(비선형 변환), 복잡하게 섞인 점들도 직선 하나로 싹둑 나눌 수 있게 됩니다.
-   이것이 딥러닝이 **"사람의 미묘한 취향"**을 구분해내는 비결입니다.

## 9. Q&A: "층층이 쌓는다는 게 이해가 안 되는데?"

**"정보를 점점 더 고급스럽게 가공한다"**는 뜻입니다.
마치 **탐정 수사팀**이 사건을 해결하는 과정과 똑같습니다.

### 탐정 수사팀 비유 (3층 구조)

**1층 (신입 형사들 - Layer 1)**: **단순한 사실 수집**
-   입력: 현장 사진, 지문, CCTV
-   하는 일: "여기 발자국이 있네?", "창문이 깨졌네?"
-   출력: **단편적인 단서들**

**2층 (베테랑 반장님 - Layer 2)**: **단서 연결 (추론)**
-   입력: 신입 형사들이 가져온 단서들
-   하는 일: "발자국이 280mm이고 창문이 밖에서 깨졌으니, 범인은 키 큰 남자다."
-   출력: **구체적인 정황**

**3층 (경찰서장 - Layer 3)**: **최종 결론 (판단)**
-   입력: 반장님의 보고서
-   하는 일: "모든 정황을 보니 범인은 정원사가 확실해. 체포해!"
-   출력: **최종 예측 (범인)**

### 결론
"층을 쌓는다"는 것은 **앞 단계의 결과물을 받아서, 더 깊이 생각하는 단계**를 계속 추가한다는 뜻입니다.
-   1층만 있으면: "발자국이 있다" (끝)
-   3층까지 쌓으면: "발자국이 있으니 -> 키 큰 남자고 -> 범인은 정원사다" (고차원적 추론)

-   3층까지 쌓으면: "발자국이 있으니 -> 키 큰 남자고 -> 범인은 정원사다" (고차원적 추론)

## 10. Q&A: "그럼 생각할 때마다 층이 쌓인다고 생각하면 되는 거야?"

**맞습니다!** 정확히는 **"생각의 깊이가 깊어진다"**고 보시면 됩니다.

### 책 읽기 비유 (Abstraction)
우리가 글을 읽을 때 뇌에서 일어나는 과정과 비슷합니다.

1.  **Layer 1 (글자 인식)**:
    -   `ㄱ`, `ㅏ`, `ㄴ`, `ㅏ`... (그냥 선과 점으로 보임)
2.  **Layer 2 (단어 인식)**:
    -   "가나다", "사과", "맛있다" (의미가 생김)
3.  **Layer 3 (문장 이해)**:
    -   "사과는 맛있다." (정보가 됨)
4.  **Layer 4 (문맥/심층 이해)**:
    -   "백설공주가 사과를 먹었다." -> **"아, 저거 독사과인데! 큰일 났다!"** (숨겨진 의미 파악)

### 결론
층이 얕으면(Layer 1) 그냥 "빨간 동그라미(사과)"로만 보이지만,
층이 깊어지면(Layer 4) "위험한 물건(독사과)"이라는 **고차원적인 의미**를 파악할 수 있게 됩니다.

## 11. Q&A: "단서 연결(추론)은 코드 어디에 있는 거지?"

**`mlp_modules` 안에 있는 `nn.Linear`와 `ReLU`가 바로 그 역할을 합니다!**

### 코드 vs 탐정 비유 매핑

```python
# 1. 사실 수집 (Input Layer)
# user_embedding, item_embedding
# "철수다", "어벤져스다" (단순 정보)

# 2. 단서 연결 & 추론 (Hidden Layers)
# 여기가 바로 "베테랑 형사"들이 일하는 곳입니다!
mlp_modules.append(nn.Linear(input_size, 64)) # 정보를 섞고 (단서 연결)
mlp_modules.append(nn.ReLU())                 # 판단하고 (비선형 추론)

mlp_modules.append(nn.Linear(64, 32))         # 더 깊이 연결하고
mlp_modules.append(nn.ReLU())                 # 더 깊이 판단하고

# 3. 최종 결론 (Output Layer)
predict_layer = nn.Linear(32, 1)              # "범인은 정원사다!" (점수 예측)
```

### 핵심 원리
-   **`nn.Linear` (섞기)**: "발자국" 정보와 "창문" 정보를 **수학적으로 섞습니다(Matrix Multiplication)**. 이게 바로 **"단서를 연결"**하는 과정입니다.
-   **`ReLU` (판단)**: 섞은 정보가 쓸모없으면 버리고(0), 중요하면 남깁니다. 이게 **"추론"**입니다.

## 12. Q&A: "리니어 하나하나가 그럼 레이어구나?"

**딩동댕! 정확합니다.**

### 엄밀한 정의 (Deep Learning Terminology)
보통 딥러닝에서는 **`Linear` + `ReLU`**를 묶어서 **"하나의 은닉층(Hidden Layer)"**이라고 부릅니다.

-   **Linear (변환)**: 데이터를 섞어서 새로운 특징을 만듦. (형사가 단서를 연결함)
-   **ReLU (판단)**: 의미 없는 정보를 버림. (형사가 헛다리 짚은 걸 제외함)

이 두 과정이 합쳐져야 비로소 **"유의미한 생각의 한 단계"**가 완성되기 때문입니다.
그래서 코드를 보면 `Linear` 바로 뒤에 항상 `ReLU`가 따라다니는 것을 볼 수 있습니다. (바늘 가는데 실 가듯이!)

## 13. Q&A: "그러니까 ReLU에서 클러스터를 생성했지만 거기에서 의미 없는 걸 버린다는 거지?"

**절반은 맞고 절반은 틀렸습니다!** 역할 분담을 조금 더 명확히 해드릴게요.

### 역할 분담 (Role Play)
1.  **Linear (생성자)**: 클러스터(특징)를 **만드는** 역할입니다.
    -   "자, 여기 액션성, 로맨스성, 배우 점수 다 섞어서 새로운 특징 64개를 만들어봤어!"
2.  **ReLU (필터/검문소)**: 만든 것 중에서 의미 없는 걸 **버리는** 역할입니다.
    -   "음, 이 3번째 특징은 음수네? 쓸모없어. 0으로 바꿔."
    -   "5번째 특징은 양수네? 이건 중요한 신호야. 통과!"

### 조각가 비유
-   **Linear**: 찰흙을 붙여서 대략적인 **형태를 빚습니다**. (Creation)
-   **ReLU**: 필요 없는 부분을 **깎아냅니다**. (Selection)

이 두 작업이 반복되면서 정교한 조각상(취향 분석)이 완성되는 것입니다.

## 14. Q&A: "그런데 음수가 만들어질 수가 있을까?"

**네, 무조건 만들어집니다!** 그리고 아주 중요합니다.

### 음수가 생기는 이유
수식 $y = Wx + b$에서 **가중치($W$)**가 음수일 수 있기 때문입니다.

### 예시: "공포 영화를 싫어하는 철수"
1.  **입력 ($x$)**: "이 영화는 공포 영화다" (+1)
2.  **가중치 ($W$)**: "철수는 공포 영화를 **싫어한다**" (**-5**)
3.  **계산 ($Wx$)**: $1 \times -5 = \mathbf{-5}$ (음수 발생!)

### ReLU의 역할
-   **Linear 결과**: -5 (철수가 싫어함)
-   **ReLU 결과**: 0 (차단)
-   **의미**: "철수가 싫어하는 장르니까, 추천 점수에 **기여하지 못하게 막아라!**"

즉, **음수**는 **"싫어한다"** 또는 **"관련 없다"**는 강력한 신호이며, ReLU는 이를 감지해서 신호를 꺼버리는(0) 역할을 합니다.

## 15. Q&A: "그런데 철수는 '어떤 영화'를 싫어한다고 해야지, '공포'라는 건 모르잖아?"

**정말 예리하십니다!** 우리가 모델에게 "이건 공포 영화야"라고 알려준 적이 없으니까요.

### 모델이 "공포"를 알아내는 방법 (Blind Taste Test)
모델은 **"눈치"**로 알아냅니다.

1.  **관찰**: 철수가 `컨저링`, `링`, `곤지암`을 모두 싫어했습니다. (평점 1점)
2.  **패턴 발견**: 모델은 이 세 영화의 공통점을 찾습니다.
    -   "뭔지는 모르겠지만, 이 세 영화는 **'특성 3번'** 수치가 엄청 높네?"
3.  **추론**: "그리고 철수는 **'특성 3번'**이 높은 영화만 보면 싫어하네?"
4.  **결론**: "그럼 앞으로 **'특성 3번'**이 높은 영화가 들어오면 철수한테 추천하지 말아야겠다!"

여기서 **'특성 3번'**이 바로 우리가 부르는 **"공포(Horror)"**입니다.
모델은 "공포"라는 단어는 모르지만, **"철수가 싫어하는 그 느낌(Latent Factor)"**은 기가 막히게 찾아냅니다.

## 16. 면접용 요약 멘트

> "MF는 내적(Dot Product)을 통해 사용자와 아이템의 선형적인 관계만 학습할 수 있다는 한계가 있습니다.
> 반면 NCF는 **MLP(Multi-Layer Perceptron)**를 도입하여 비선형적인 상호작용까지 학습할 수 있습니다.
> 제 프로젝트에서는 이 둘을 결합한 **NeuMF** 구조를 사용하여, MF의 일반화 능력과 MLP의 표현력을 모두 활용했습니다."
